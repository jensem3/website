<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Jen Semler is a graduate student in philosophy at Oxford studying AI ethics">
        <meta name="author" content="Jen Semler">
        <meta name="keywords" content="philosophy, AI ethics, Oxford">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Jen Semler | Research</title>
        <link rel="stylesheet" href="style.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto Condensed">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">



    <body>
        <header>
            <h1><a href ="index.html" class="logo">Jen Semler</a></h1>
            <input type="checkbox" id="nav-toggle" class="nav-toggle">
            <nav>
                <ul>
                    <li><a href="projects.html">Research</a></li>
                    <li><a href="Semler_CV.pdf" target="_blank">CV</a></li>
                </ul>
            </nav>
            <label for="nav-toggle" class="nav-toggle-label">
                <span></span>
            </label>
                   
        </header>

        <main class="research-grid">

            <div class="project">
                <h2>Research Overview</h2>
                <hr>
                <p>
                    Imagine you&#39;ve been fired from your job by a reliable and competent decision-maker. 
                    This decision-maker reached their conclusion in the right way&mdash;and they can provide reasons and explanations for their decision. 
                    Does it matter who, or what, the decision-maker is? 
                    The guiding claim of my research is that it matters whether morally laden decisions are made by moral agents. 
                    My work considers what makes something a moral agent, which entities are moral agents, and why we should care about moral agency—all with a focus on AI. 
                    These questions form three interrelated research strands.
                </p>
                <p>
                    <h3>Non-Prototypical Moral Agency</h3>
                    This theoretical strand of my research concerns the nature of moral agency, with a particular focus on moral agents (and potential moral agents) beyond the prototypical adult human. 
                    Broadly, on my account, there are different types of moral agency&mdash;and while there are certain necessary conditions for each type, these conditions can be instantiated in a variety of ways. 
                </p>
                <p> 
                    <h3>Applied Ethics of Technology</h3>
                    This applied strand of my research concerns how we ought to integrate AI systems into our moral practices. 
                    The papers in this strand focus on how our use of AI in moral decision-making contexts should be limited by AI systems’ lack of moral agency. 
                <p>
                    <h3>Empirically Engaged Philosophy of Moral Agency</h3>
                    This empirical strand of my research concerns the capabilities of AI systems (empirically informed and technically grounded philosophy) and the ways in which humans think about moral agency (experimental philosophy). 
                </p>
            </div>

            <div class="project">
                <h2>Publications</h2>
                <hr>
                <h3>Journal Articles</h3>
                <p>
                    <b>Moral Agency Without Consciousness</b>
                        | <a href="https://philpapers.org/archive/SEMMAW.pdf" target="_blank">penultimate draft</a>
                    <br>forthcoming in <em>Canadian Journal of Philosophy</em> 
                </p>
                <p>
                    <b>A Timing Problem for Instrumental Convergence</b>
                        | <a href="https://philpapers.org/archive/SOUATP-3.pdf" target="_blank">penultimate draft</a>
                    <br>with R. Southan and H. Ward
                    <br>forthcoming in <em>Philosophical Studies</em>
                </p>
                <p>
                    <b>Recent Experimental Work on &#39;Ought&#39; Implies &#39;Can&#39;</b>
                    | <a href="https://doi.org/10.1111/phc3.12619" target="_blank">published version</a> 
                    <br>with P. Henne
                    <br><em>Philosophy Compass</em> (2019)
                </p>
                <p>
                    <b>Against some Recent Arguments for &#39;Ought&#39; Implies &#39;Can&#39;: Reasons, Deliberation, Trying, and Furniture</b>
                    | <a href="https://doi.org/10.1007/s11406-017-9944-7" target="_blank">published version</a> 
                    <br>with P. Henne, V. Chituc, F. De Brigard, & W. Sinnott-Armstrong
                    <br><em>Philosophia</em> (2019)
                </p>
                <h3>DPhil (PhD) Dissertation</h3>
                <p>
                    <b>On Artificial Moral Agency</b>
                    | <a href="Dissertation_Abstract.pdf" target="_blank">abstract</a>
                    <br>University of Oxford (2025)
                </p>
                <h3>Textbook Contributions</h3>
                <p>
                    <b>Corporate Human Rights Obligations</b> (under contract)
                    <br>case study for the<q>Human Rights</q> chapter in <em>Issues in Political Theory</em>, edited by R. Jubb & P. Tomlin, Oxford University Press
                </p>
                <p>
                    <b>Lockdowns</b> (under contract)
                    <br>case study for the<q>Liberty</q> chapter in <em>Issues in Political Theory</em>, edited by R. Jubb & P. Tomlin, Oxford University Press
                </p>
            </div>

        </main> 

        <footer>
        </footer>


    </body>

</html>
