<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Jen Semler is a graduate student in philosophy at Oxford studying AI ethics">
        <meta name="author" content="Jen Semler">
        <meta name="keywords" content="philosophy, AI ethics, Oxford">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Jen Semler | Research</title>
        <link rel="stylesheet" href="style.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto Condensed">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">



    <body>
        <header>
            <h1><a href ="index.html" class="logo">Jen Semler</a></h1>
            <input type="checkbox" id="nav-toggle" class="nav-toggle">
            <nav>
                <ul>
                    <li><a href="projects.html">Research</a></li>
                    <li><a href="Semler_CV.pdf" target="_blank">CV</a></li>
                </ul>
            </nav>
            <label for="nav-toggle" class="nav-toggle-label">
                <span></span>
            </label>
                   
        </header>

        <main class="research-grid">

            <div class="project">
                <h2>Research Overview</h2>
                <hr>
                <p>
                    Imagine you&#39;ve been fired from your job by a reliable and competent decision-maker. 
                    This decision-maker reached their conclusion in the right way&mdash;and they can provide reasons and explanations for their decision. 
                    Does it matter who, or what, the decision-maker is? 
                    The guiding claim of my research is that it matters whether morally laden decisions are made by moral agents. 
                    My work considers what makes something a moral agent, which entities are moral agents, and why we should care about moral agency—all with a focus on AI. 
                    These questions form three interrelated research strands.
                </p>
                <p>
                    The first strand&mdash;<b>Non-Prototypical Moral Agency</b>&mdash;concerns the nature of moral agency, with a particular focus on moral agents (and potential moral agents) beyond the prototypical adult human. 
                    Broadly, on my account, there are different types of moral agency—and while there are certain necessary conditions for each type, these conditions can be instantiated in a variety of ways. 
                </p>
                <p> 
                    The second strand&mdash;<b>Applied Ethics of Technology</b>&mdash;considers how we ought to integrate AI systems into our moral practices. 
                    The papers in this strand focus on how our use of AI in moral decision-making contexts should be limited by AI systems&#39; lack of moral agency. 
                <p>
                    The third strand&mdash;<b>Empirically Engaged Philosophy of Moral Agency</b>&mdash;evaluates the capabilities of AI systems and the ways in which humans think about moral agency. 
                    I engage with machine learning research to understand whether there is evidence of AI systems having philosophically relevant capacities.
                    I also conduct experimental philosophy projects.
                </p>
            </div>

            <div class="project">
                <h2>Publications</h2>
                <hr>
                <h3>Journal Articles</h3>
                <p>
                    Recent Experimental Work on &#39;Ought&#39; Implies &#39;Can&#39;
                    | <a href="https://doi.org/10.1111/phc3.12619" target="_blank">link</a> 
                    | <a href="Semler_Henne_2019.pdf" target="_blank">pdf</a>
                    <br>(with P. Henne)
                    <br><em>Philosophy Compass</em> (2019)
                </p>
                <p>
                    Against some Recent Arguments for &#39;Ought&#39; Implies &#39;Can&#39;: Reasons, Deliberation, Trying, and Furniture
                    | <a href="https://doi.org/10.1007/s11406-017-9944-7" target="_blank">link</a> 
                    | <a href="Henne_Semler_etal_2018.pdf" target="_blank">pdf</a>
                    <br>(with P. Henne, V. Chituc, F. De Brigard, & W. Sinnott-Armstrong)
                    <br><em>Philosophia</em> (2018)
                </p>
                <h3>Textbook Contributions</h3>
                <p>
                    Corporate Human Rights Obligations (under contract)
                    <br>case study for the<q>Human Rights</q> chapter in <em>Issues in Political Theory</em> (edited by R. Jubb & P. Tomlin), Oxford University Press
                </p>
                <p>
                    Lockdowns (under contract)
                    <br>case study for the<q>Liberty</q> chapter in <em>Issues in Political Theory</em> (edited by R. Jubb & P. Tomlin), Oxford University Press
                </p>
            </div>

        </main> 

        <footer>
            <a href="https://twitter.com/jensemler" target="_blank" class="fa fa-twitter"></a>
            <a href="mailto:jen.semler@philosophy.ox.ac.uk" class="fa fa-envelope"></a>
            <a href="https://www.linkedin.com/in/jensemler/" target="_blank" class="fa fa-linkedin"></a>
        </footer>


    </body>

</html>
